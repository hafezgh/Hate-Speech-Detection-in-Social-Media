# Parameters for the example pipeline. Feel free to delete these once you
# remove the example pipeline from hooks.py and the example nodes in
# `src/pipelines/`
dataset: davidson
train_size_ratio: 0.8
test_size_ratio: 0.5
model_name: bert-base-uncased
sentence_max_length: 36
tokenize_batch_size: 4
device: cpu
BERT_model: shallow_fc
training_type: transformers
multilingual_task: task1
learning_rate: 2e-5
train_args:
  output_dir: ./results          # output directory
  num_train_epochs: 3              # total number of training epochs
  per_device_train_batch_size: 64  # batch size per device during training
  per_device_eval_batch_size: 64   # batch size for evaluation
#  warmup_steps: 500                # number of warmup steps for learning rate scheduler
#  weight_decay: 0.01               # strength of weight decay
  logging_dir: ./transformers_logs            # directory for storing logs
  load_best_model_at_end: True     # load the best model when finished training (default metric is loss)
  # but you can specify `metric_for_best_model` argument to change to accuracy or other metric
#  logging_steps: 400               # log & save weights each logging_steps
#  save_steps: 400
  evaluation_strategy: epoch
  save_strategy: epoch
