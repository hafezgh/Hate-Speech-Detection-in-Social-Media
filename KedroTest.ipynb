{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KedroTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h88slt1K0WqW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "! pip install kedro transformers datasets emoji wandb\n",
        "! git clone https://github.com/hafezgh/Hate-Speech-Detection-in-Social-Media.git\n",
        "os.chdir('/content/Hate-Speech-Detection-in-Social-Media')\n",
        "\n",
        "! git checkout kedro-test\n",
        "os.chdir('/content/Hate-Speech-Detection-in-Social-Media/Irirs_kedro/get-started')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kedro run --params BERT_model:cnn,training_type:transformers,device:cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3KFc4w69sI",
        "outputId": "97d987e3-bdb7-4819-9215-b6e2203b2b85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jsonschema/compat.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import MutableMapping, Sequence  # noqa\n",
            "2022-01-11 15:19:04,511 - kedro.framework.session.store - INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\n",
            "2022-01-11 15:19:04,532 - kedro.framework.session.session - INFO - ** Kedro project get-started\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/utils/module_paths.py:29: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "/usr/local/lib/python3.7/dist-packages/kedro/framework/context/context.py:488: UserWarning: Credentials not found in your Kedro project config.\n",
            "No files found in ['/content/Hate-Speech-Detection-in-Social-Media/Irirs_kedro/get-started/conf/base', '/content/Hate-Speech-Detection-in-Social-Media/Irirs_kedro/get-started/conf/local'] matching the glob pattern(s): ['credentials*', 'credentials*/**', '**/credentials*']\n",
            "  warn(f\"Credentials not found in your Kedro project config.\\n{str(exc)}\")\n",
            "/usr/local/lib/python3.7/dist-packages/kedro/io/data_catalog.py:194: DeprecationWarning: The transformer API will be deprecated in Kedro 0.18.0.Please use Dataset Hooks to customise the load and save methods.For more information, please visithttps://kedro.readthedocs.io/en/stable/07_extend_kedro/02_hooks.html\n",
            "  DeprecationWarning,\n",
            "2022-01-11 15:19:08,521 - kedro.io.data_catalog - INFO - Loading data from `labeled_dataset` (CSVDataSet)...\n",
            "2022-01-11 15:19:08,563 - kedro.pipeline.node - INFO - Running node: clean: clean_data([labeled_dataset]) -> [cleaned_dataset]\n",
            "2022-01-11 15:19:09,563 - kedro.io.data_catalog - INFO - Saving data to `cleaned_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:09,565 - kedro.runner.sequential_runner - INFO - Completed 1 out of 6 tasks\n",
            "2022-01-11 15:19:09,565 - kedro.io.data_catalog - INFO - Loading data from `cleaned_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:09,565 - kedro.io.data_catalog - INFO - Loading data from `params:model_name` (MemoryDataSet)...\n",
            "2022-01-11 15:19:09,565 - kedro.io.data_catalog - INFO - Loading data from `params:tokenize_batch_size` (MemoryDataSet)...\n",
            "2022-01-11 15:19:09,566 - kedro.io.data_catalog - INFO - Loading data from `params:tokenizer_max_length` (MemoryDataSet)...\n",
            "2022-01-11 15:19:09,566 - kedro.pipeline.node - INFO - Running node: prepare: prepare_data([cleaned_dataset,params:model_name,params:tokenize_batch_size,params:tokenizer_max_length]) -> [dataset]\n",
            "100% 6196/6196 [00:12<00:00, 487.48ba/s]\n",
            "2022-01-11 15:19:24,841 - kedro.io.data_catalog - INFO - Saving data to `dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,843 - kedro.runner.sequential_runner - INFO - Completed 2 out of 6 tasks\n",
            "2022-01-11 15:19:24,843 - kedro.io.data_catalog - INFO - Loading data from `dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,843 - kedro.io.data_catalog - INFO - Loading data from `params:train_size_ratio` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,843 - kedro.io.data_catalog - INFO - Loading data from `params:test_size_ratio` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,844 - kedro.pipeline.node - INFO - Running node: split: split_data([dataset,params:train_size_ratio,params:test_size_ratio]) -> [train_dataset,eval_dataset,test_dataset]\n",
            "2022-01-11 15:19:24,871 - kedro.io.data_catalog - INFO - Saving data to `train_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,872 - kedro.io.data_catalog - INFO - Saving data to `eval_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,873 - kedro.io.data_catalog - INFO - Saving data to `test_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,874 - kedro.runner.sequential_runner - INFO - Completed 3 out of 6 tasks\n",
            "2022-01-11 15:19:24,874 - kedro.io.data_catalog - INFO - Loading data from `params:BERT_model` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,875 - kedro.io.data_catalog - INFO - Loading data from `params:model_name` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,875 - kedro.io.data_catalog - INFO - Loading data from `params:training_type` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,875 - kedro.io.data_catalog - INFO - Loading data from `params:train_args` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,875 - kedro.io.data_catalog - INFO - Loading data from `params:device` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,875 - kedro.io.data_catalog - INFO - Loading data from `train_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,876 - kedro.io.data_catalog - INFO - Loading data from `eval_dataset` (MemoryDataSet)...\n",
            "2022-01-11 15:19:24,876 - kedro.pipeline.node - INFO - Running node: train: train([params:BERT_model,params:model_name,params:training_type,params:train_args,params:device,train_dataset,eval_dataset]) -> [trained_model]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "***** Running training *****\n",
            "  Num examples = 19826\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 930\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/juanjose/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/juanjose/huggingface/runs/38bbgiiz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/Hate-Speech-Detection-in-Social-Media/Irirs_kedro/get-started/wandb/run-20220111_151959-38bbgiiz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            " 33% 310/930 [06:39<12:41,  1.23s/it]***** Running Evaluation *****\n",
            "  Num examples = 2478\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/39 [00:00<00:03,  9.77it/s]\u001b[A\n",
            "  8% 3/39 [00:00<00:05,  6.83it/s]\u001b[A\n",
            " 10% 4/39 [00:00<00:06,  5.83it/s]\u001b[A\n",
            " 13% 5/39 [00:00<00:06,  5.45it/s]\u001b[A\n",
            " 15% 6/39 [00:01<00:06,  5.15it/s]\u001b[A\n",
            " 18% 7/39 [00:01<00:06,  5.00it/s]\u001b[A\n",
            " 21% 8/39 [00:01<00:06,  4.84it/s]\u001b[A\n",
            " 23% 9/39 [00:01<00:06,  4.89it/s]\u001b[A\n",
            " 26% 10/39 [00:01<00:05,  4.85it/s]\u001b[A\n",
            " 28% 11/39 [00:02<00:05,  4.81it/s]\u001b[A\n",
            " 31% 12/39 [00:02<00:05,  4.77it/s]\u001b[A\n",
            " 33% 13/39 [00:02<00:05,  4.64it/s]\u001b[A\n",
            " 36% 14/39 [00:02<00:05,  4.65it/s]\u001b[A\n",
            " 38% 15/39 [00:02<00:05,  4.68it/s]\u001b[A\n",
            " 41% 16/39 [00:03<00:04,  4.72it/s]\u001b[A\n",
            " 44% 17/39 [00:03<00:04,  4.74it/s]\u001b[A\n",
            " 46% 18/39 [00:03<00:04,  4.68it/s]\u001b[A\n",
            " 49% 19/39 [00:03<00:04,  4.72it/s]\u001b[A\n",
            " 51% 20/39 [00:04<00:04,  4.72it/s]\u001b[A\n",
            " 54% 21/39 [00:04<00:03,  4.70it/s]\u001b[A\n",
            " 56% 22/39 [00:04<00:03,  4.70it/s]\u001b[A\n",
            " 59% 23/39 [00:04<00:03,  4.73it/s]\u001b[A\n",
            " 62% 24/39 [00:04<00:03,  4.77it/s]\u001b[A\n",
            " 64% 25/39 [00:05<00:02,  4.82it/s]\u001b[A\n",
            " 67% 26/39 [00:05<00:02,  4.83it/s]\u001b[A\n",
            " 69% 27/39 [00:05<00:02,  4.83it/s]\u001b[A\n",
            " 72% 28/39 [00:05<00:02,  4.77it/s]\u001b[A\n",
            " 74% 29/39 [00:05<00:02,  4.76it/s]\u001b[A\n",
            " 77% 30/39 [00:06<00:01,  4.73it/s]\u001b[A\n",
            " 79% 31/39 [00:06<00:01,  4.73it/s]\u001b[A\n",
            " 82% 32/39 [00:06<00:01,  4.72it/s]\u001b[A\n",
            " 85% 33/39 [00:06<00:01,  4.78it/s]\u001b[A\n",
            " 87% 34/39 [00:06<00:01,  4.80it/s]\u001b[A\n",
            " 90% 35/39 [00:07<00:00,  4.77it/s]\u001b[A\n",
            " 92% 36/39 [00:07<00:00,  4.79it/s]\u001b[A\n",
            " 95% 37/39 [00:07<00:00,  4.82it/s]\u001b[A\n",
            " 97% 38/39 [00:07<00:00,  4.84it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.23338451981544495, 'eval_accuracy': 0.9120258272800645, 'eval_f1_score_weighted': 0.8908321393484098, 'eval_precision_weighted': 0.8940452745628847, 'eval_recall_weighted': 0.9120258272800645, 'eval_f1_score_macro': 0.6545666851289181, 'eval_precision_macro': 0.7770914382442907, 'eval_recall_macro': 0.6616513107907575, 'eval_runtime': 8.2545, 'eval_samples_per_second': 300.198, 'eval_steps_per_second': 4.725, 'epoch': 1.0}\n",
            " 33% 310/930 [06:47<12:41,  1.23s/it]\n",
            "100% 39/39 [00:08<00:00,  4.84it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./results/checkpoint-310\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            " 48% 444/930 [09:46<10:31,  1.30s/it]2022-01-11 15:29:49,694 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step.\n",
            "\n",
            "Aborted!\n",
            " 48% 444/930 [09:46<10:41,  1.32s/it]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 530... (failed 1). Press ctrl-c to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/f1_score_macro ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/f1_score_weighted ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/precision_macro ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/precision_weighted ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/recall_macro ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/recall_weighted ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/epoch ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.91203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/f1_score_macro 0.65457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/f1_score_weighted 0.89083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.23338\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/precision_macro 0.77709\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/precision_weighted 0.89405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/recall_macro 0.66165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/recall_weighted 0.91203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 8.2545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 300.198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 4.725\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 310\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m./results\u001b[0m: \u001b[34mhttps://wandb.ai/juanjose/huggingface/runs/38bbgiiz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220111_151959-38bbgiiz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the example pipeline. Feel free to delete these once you\n",
        "# remove the example pipeline from hooks.py and the example nodes in\n",
        "# `src/pipelines/`\n",
        "train_size_ratio: 0.8\n",
        "test_size_ratio: 0.5\n",
        "model_name: bert-base-uncased\n",
        "tokenizer_max_length: 36\n",
        "tokenize_batch_size: 256\n",
        "device: cpu\n",
        "BERT_model: shallow_fc\n",
        "training_type: transformers\n",
        "train_args:\n",
        "  output_dir: ./results          # output directory\n",
        "  num_train_epochs: 1              # total number of training epochs\n",
        "  per_device_train_batch_size: 256  # batch size per device during training\n",
        "  per_device_eval_batch_size: 256   # batch size for evaluation\n",
        "#  warmup_steps: 500                # number of warmup steps for learning rate scheduler\n",
        "#  weight_decay: 0.01               # strength of weight decay\n",
        "  logging_dir: ./transformers_logs            # directory for storing logs\n",
        "  load_best_model_at_end: True     # load the best model when finished training (default metric is loss)\n",
        "  # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "#  logging_steps: 400               # log & save weights each logging_steps\n",
        "#  save_steps: 400\n",
        "  evaluation_strategy: epoch\n",
        "  save_strategy: epoch\n"
      ],
      "metadata": {
        "id": "rtL1c9mqjDax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}